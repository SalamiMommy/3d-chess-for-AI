"""Move Cache - PURE CACHING ONLY, NO MOVE GENERATION.

This module ONLY caches legal moves. It does NOT generate moves.
All move generation happens in generator.py.
"""

import numpy as np
from numba import njit
from typing import Optional, Dict, Any, Union
from dataclasses import dataclass
import threading
import logging
from collections import OrderedDict

logger = logging.getLogger(__name__)

from game3d.common.shared_types import (
    COORD_DTYPE, INDEX_DTYPE, BOOL_DTYPE, VOLUME, MOVE_DTYPE as MOVE_DTYPE,
    Color, SIZE
)

@njit(cache=True)
def _find_move_by_key(keys: np.ndarray, target_key: int) -> int:
    mask = keys == target_key
    matches = np.where(mask)[0]
    return matches[0] if matches.size > 0 else -1

@dataclass
class MoveCacheConfig:
    """Configuration for move cache."""
    max_cache_size: int = 10000
    enable_transposition_table: bool = True

class MoveCache:
    """
    Pure caching layer for legal moves.

    Responsibilities:
    1. Cache legal moves generated by generator.py
    2. Invalidate cache when board state changes
    3. Track cache statistics

    Does NOT:
    - Generate moves (that's generator.py's job)
    - Validate moves (that's generator.py's job)
    - Apply moves (that's gamestate.py's job)
    """
    def __init__(self, cache_manager, config=None):
        self.cache_manager = cache_manager
        self.config = config or MoveCacheConfig()
        self._lock = threading.RLock()

        # âœ… FIXED: Initialize to current board generation, not -1
        current_gen = getattr(cache_manager.board, 'generation', 0)
        self._cached_moves = [None, None]
        self._cache_generation = 0
        self._board_generation = current_gen  # Use current generation

        stats_dtype = np.dtype([
            ('cache_hits', INDEX_DTYPE),
            ('cache_misses', INDEX_DTYPE),
            ('total_moves_cached', INDEX_DTYPE)
        ])
        self._stats = np.zeros(1, dtype=stats_dtype)[0]
        self._affected_pieces = set()
        self._piece_board_generations = {}
        self._board_generation_per_color = [current_gen, current_gen]

        # Initialize missing attributes
        self._affected_coord_keys_list = []  # âœ… Use list for O(1) append
        self._affected_color_idx_list = []
        self._affected_coord_keys = np.empty(0, dtype=np.int64)
        self._affected_color_idx = np.empty(0, dtype=np.int8)
        
        # âœ… OPTIMIZED: Use OrderedDict for LRU cache
        self._piece_moves_cache = OrderedDict()

        # Reverse Move Map: Square Key -> Set of Piece Keys
        # Used for incremental updates to find pieces attacking a square
        self._reverse_map: Dict[int, set] = {}
        # Track which squares a piece targets to allow efficient removal
        self._piece_targets: Dict[tuple, set] = {}

        # âœ… NEW: LRU tracking and size limits to prevent memory explosion
        self._max_piece_entries = 1000
        self._prune_triggered = 0  # Statistics

    def get_cached_moves(self, color: int) -> Optional[np.ndarray]:
        """Retrieve cached moves for color.

        âœ… CRITICAL FIX: Don't invalidate on generation change alone.
        The incremental update system (affected_pieces tracking) handles
        selective invalidation. Only invalidate if cache is empty or
        if we have affected pieces that need regeneration.
        """
        with self._lock:
            color_idx = 0 if color == Color.WHITE else 1

            # Cache miss if no moves stored
            if self._cached_moves[color_idx] is None:
                # logger.debug(f"MoveCache miss for {'WHITE' if color_idx == 0 else 'BLACK'} (empty cache)")
                self._stats['cache_misses'] += 1
                return None

            # âœ… NEW LOGIC: Cache is valid if:
            # 1. We have cached moves AND
            # 2. No affected pieces for this color (incremental system tracks changes)
            affected = self.get_affected_pieces(color)

            if affected.size > 0:
                # Partial invalidation - let generator handle incremental update
                # logger.debug(
                #     f"MoveCache partial miss for {'WHITE' if color_idx == 0 else 'BLACK'}: "
                #     f"{affected.size} pieces affected (incremental update needed)"
                # )
                self._stats['cache_misses'] += 1
                return None

            # Cache hit - no affected pieces means moves are still valid
            self._stats['cache_hits'] += 1
            return self._cached_moves[color_idx]

    def store_moves(self, color: int, moves: np.ndarray) -> None:
        """Store generated moves with validation."""
        with self._lock:
            color_idx = 0 if color == Color.WHITE else 1

            # ðŸš¨ NEVER cache empty moves unless truly stalemate
            if moves.size == 0:
                piece_count = len(self.cache_manager.occupancy_cache.get_positions(color))
                if piece_count > 0:
                    logger.error(
                        f"ðŸš¨ REFUSING to cache 0 moves for {piece_count} pieces. "
                        f"This indicates generation failure."
                    )
                    self._cached_moves[color_idx] = None  # Force retry next time
                    return

            self._cached_moves[color_idx] = moves.copy()
            current_gen = getattr(self.cache_manager.board, 'generation', 0)
            self._board_generation_per_color[color_idx] = current_gen
            self._cache_generation += 1
            self._stats['total_moves_cached'] += len(moves)

            # LOUD logging for empty move detection
            if moves.size == 0:
                logger.warning(f"Storing empty moves for color {color} - stalemate detected")

    def invalidate(self) -> None:
        """Invalidate all cached moves."""
        with self._lock:
            self._cached_moves[0] = None  # âœ… FIXED: Use None
            self._cached_moves[1] = None
            self._cache_generation += 1

    def get_statistics(self) -> Dict[str, Any]:
        """Get cache statistics."""
        with self._lock:
            total_lookups = self._stats['cache_hits'] + self._stats['cache_misses']
            hit_rate = self._stats['cache_hits'] / max(total_lookups, 1)

            # âœ… FIXED: Handle None values gracefully
            white_moves = 0 if self._cached_moves[0] is None else len(self._cached_moves[0])
            black_moves = 0 if self._cached_moves[1] is None else len(self._cached_moves[1])

            return {
                'cache_hits': self._stats['cache_hits'],
                'cache_misses': self._stats['cache_misses'],
                'hit_rate': hit_rate,
                'total_moves_cached': self._stats['total_moves_cached'],
                'white_moves_cached': white_moves,
                'black_moves_cached': black_moves,
                'reverse_map_size': len(self._reverse_map),
                'piece_moves_cache_size': len(self._piece_moves_cache),
                'prune_operations': self._prune_triggered  # âœ… NEW: Track pruning
            }

    def clear(self) -> None:
        """Clear all cached data."""
        with self._lock:
            self.invalidate()
            self._stats['cache_hits'] = 0
            self._stats['cache_misses'] = 0
            self._stats['total_moves_cached'] = 0
            self._piece_moves_cache.clear()
            self._reverse_map.clear()
            self._piece_targets.clear()
            self._affected_coord_keys = np.empty(0, dtype=np.int64)
            self._affected_color_idx = np.empty(0, dtype=np.int8)
            self._affected_coord_keys_list = []  # âœ… Clear lists
            self._affected_color_idx_list = []


    def mark_piece_invalid(self, color: int, coord_key: Union[int, bytes]) -> None:
        """Mark piece for regeneration - USE INTEGER KEYS."""
        color_idx = 0 if color == Color.WHITE else 1

        if isinstance(coord_key, (int, np.integer)):
            int_key = int(coord_key)
        else:
            int_key = int.from_bytes(coord_key, 'little') if coord_key else 0

        with self._lock:
            # âœ… OPTIMIZED: Use list append instead of np.append
            self._affected_coord_keys_list.append(int_key)
            self._affected_color_idx_list.append(color_idx)

    def has_piece_moves(self, color: int, coord_key: Union[int, bytes]) -> bool:
        """Check if piece moves are cached."""
        color_idx = 0 if color == Color.WHITE else 1

        if isinstance(coord_key, (int, np.integer)):
            int_key = int(coord_key)
        else:
            int_key = int.from_bytes(coord_key, 'little') if coord_key else 0

        with self._lock:
            piece_id = (color_idx, int_key)
            if piece_id in self._piece_moves_cache:
                # Move to end to mark as recently used
                self._piece_moves_cache.move_to_end(piece_id)
                return True
            return False

    def get_piece_moves(self, color: int, coord_key: Union[int, bytes]) -> np.ndarray:
        """Retrieve cached moves for a piece."""
        color_idx = 0 if color == Color.WHITE else 1

        if isinstance(coord_key, (int, np.integer)):
            int_key = int(coord_key)
        else:
            int_key = int.from_bytes(coord_key, 'little') if coord_key else 0
        
        piece_id = (color_idx, int_key)
        
        with self._lock:
            if piece_id in self._piece_moves_cache:
                # Move to end to mark as recently used
                self._piece_moves_cache.move_to_end(piece_id)
                return self._piece_moves_cache[piece_id]
            
            return np.empty((0, 6), dtype=MOVE_DTYPE)

    def store_piece_moves(self, color: int, coord_key: Union[int, bytes], moves: np.ndarray) -> None:
        """Cache moves for a specific piece - USE INTEGER KEYS."""
        color_idx = 0 if color == Color.WHITE else 1

        # Handle both int and bytes keys
        if isinstance(coord_key, (int, np.integer)):
            int_key = int(coord_key)
        else:
            int_key = int.from_bytes(coord_key, 'little') if coord_key else 0

        piece_id = (color_idx, int_key)

        with self._lock:
            # âœ… NEW: Prune cache if exceeding limit
            if len(self._piece_moves_cache) >= self._max_piece_entries:
                self._prune_piece_cache()

            # Update Reverse Map
            self._update_reverse_map(piece_id, moves)

            # Update dictionary cache (primary storage)
            # OrderedDict handles insertion order automatically
            self._piece_moves_cache[piece_id] = moves
            # Ensure it's marked as most recently used (if it was already there)
            self._piece_moves_cache.move_to_end(piece_id)

    def _update_reverse_map(self, piece_id: tuple, moves: np.ndarray) -> None:
        """Update the reverse map for a piece."""
        # 1. Clear old entries
        if piece_id in self._piece_targets:
            old_targets = self._piece_targets[piece_id]
            for target_key in old_targets:
                if target_key in self._reverse_map:
                    self._reverse_map[target_key].discard(piece_id)
                    if not self._reverse_map[target_key]:
                        del self._reverse_map[target_key]

        # 2. Add new entries
        if moves.size > 0:
            # Vectorized key generation for targets (x, y, z) -> columns 3, 4, 5
            to_x = moves[:, 3].astype(np.int64)
            to_y = moves[:, 4].astype(np.int64)
            to_z = moves[:, 5].astype(np.int64)
            # Simple packing: x | (y << 9) | (z << 18)
            target_keys = to_x | (to_y << 9) | (to_z << 18)

            unique_targets = np.unique(target_keys)

            new_targets = set()
            for t_key in unique_targets:
                t_key_int = int(t_key)
                if t_key_int not in self._reverse_map:
                    self._reverse_map[t_key_int] = set()
                self._reverse_map[t_key_int].add(piece_id)
                new_targets.add(t_key_int)

            self._piece_targets[piece_id] = new_targets
        else:
            self._piece_targets[piece_id] = set()

    def get_pieces_targeting(self, coord_keys: np.ndarray) -> list:
        """Get all pieces targeting the given coordinates. Returns list of (color_idx, piece_key)."""
        affected_pieces = set()
        with self._lock:
            for key in coord_keys:
                key_int = int(key)
                if key_int in self._reverse_map:
                    affected_pieces.update(self._reverse_map[key_int])
        return list(affected_pieces)

    def get_affected_pieces(self, color: int) -> np.ndarray:
        """Get affected pieces as numpy array."""
        color_idx = 0 if color == Color.WHITE else 1

        # âœ… OPTIMIZED: Convert lists to arrays only when reading
        if self._affected_coord_keys_list:
            # Consolidate lists into arrays once
            self._affected_coord_keys = np.array(self._affected_coord_keys_list, dtype=np.int64)
            self._affected_color_idx = np.array(self._affected_color_idx_list, dtype=np.int8)
            # Don't clear lists yet - they may be used again soon

        mask = self._affected_color_idx == color_idx
        return self._affected_coord_keys[mask]

    def clear_affected_pieces(self, color: int) -> None:
        """Clear affected pieces after regeneration."""
        color_idx = 0 if color == Color.WHITE else 1
        self._affected_pieces = {(c_idx, key) for (c_idx, key) in self._affected_pieces
                                if c_idx != color_idx}

        # âœ… OPTIMIZED: Clear both arrays and lists
        mask = self._affected_color_idx != color_idx
        self._affected_coord_keys = self._affected_coord_keys[mask]
        self._affected_color_idx = self._affected_color_idx[mask]
        
        # Clear lists for the specified color
        indices_to_keep = [i for i, c_idx in enumerate(self._affected_color_idx_list) if c_idx != color_idx]
        self._affected_coord_keys_list = [self._affected_coord_keys_list[i] for i in indices_to_keep]
        self._affected_color_idx_list = [self._affected_color_idx_list[i] for i in indices_to_keep]

    # âœ… NEW: LRU PRUNING METHOD
    def _prune_piece_cache(self) -> None:
        """Prune oldest 20% of entries when cache exceeds limit."""
        if len(self._piece_moves_cache) <= self._max_piece_entries:
            return

        prune_start = len(self._piece_moves_cache)
        prune_target = int(self._max_piece_entries * 0.8)  # Keep 80%
        num_to_remove = prune_start - prune_target

        for _ in range(num_to_remove):
            # Remove oldest entry (first in OrderedDict)
            try:
                piece_id, _ = self._piece_moves_cache.popitem(last=False)
                
                # Clean up reverse map
                if piece_id in self._piece_targets:
                    for target_key in self._piece_targets[piece_id]:
                        if target_key in self._reverse_map:
                            self._reverse_map[target_key].discard(piece_id)
                            if not self._reverse_map[target_key]:
                                del self._reverse_map[target_key]
                    del self._piece_targets[piece_id]
            except KeyError:
                break

        prune_end = len(self._piece_moves_cache)
        self._prune_triggered += 1
        logger.debug(f"Pruned piece cache: {prune_start} -> {prune_end} entries")

def create_move_cache(cache_manager, config: Optional[MoveCacheConfig] = None) -> MoveCache:
    """Factory function to create move cache."""
    return MoveCache(cache_manager, config)

__all__ = ['MoveCache', 'MoveCacheConfig', 'create_move_cache']
